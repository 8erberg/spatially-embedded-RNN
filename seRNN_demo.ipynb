{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/8erberg/spatially-embedded-RNN/blob/main/seRNN_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# seRNN Demo: How to spatially embed an RNN\n",
        "\n",
        "In this notebook we provide a run through of the basic training routine behind spatially embedded recurrent neural networks (seRNNs) and apply structural metrics to an example seRNN. We that giving researchers access to this simple overview of our implementation will inspire other research groups to develop their own versions of spatially embedded neural networks.\n",
        "\n",
        "For further details on methods, see our the related publication: https://www.nature.com/articles/s42256-023-00748-9\n",
        "\n",
        "Note that we also provide a CodeCapsule replicating our Python environment with additional details on the training and analysis pipeline alongside the data we recorded from our networks: https://codeocean.com/capsule/2879348/tree/v2\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iHpRPtFUEpUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "EYLQp4DtmY82",
        "outputId": "593cfd31-6c82-4ebb-d478-938796df8fb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QmtNrL09EJE6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)\n",
        "print(np.__version__)"
      ],
      "metadata": {
        "id": "HqDUYn7ImKIC",
        "outputId": "fbbc1014-3d9e-41e2-d994-ec918d98ca4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n",
            "2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import Regularizer\n",
        "from tensorflow.python.keras import backend\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import scipy.spatial.distance\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow.keras as keras"
      ],
      "metadata": {
        "id": "KZPteGBRLGiR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1829)\n",
        "tf.random.set_seed(9492)"
      ],
      "metadata": {
        "id": "l2RihmF-MRm1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task / Dataset"
      ],
      "metadata": {
        "id": "0renWOfsIoFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset generator\n",
        "\n",
        "\n",
        "The function below generates multiple datasets representing our maze-like task."
      ],
      "metadata": {
        "id": "oYqJ7U8nJIcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class mazeGeneratorI():\n",
        "    '''\n",
        "    Objects of the mazeGeneratorI class can create numpy and tf datasets of the first choice of the maze task.\n",
        "    Task structure:\n",
        "        Goal presentation, followed by delay period, followed by choice options.\n",
        "    Response:\n",
        "        One response required from agent at end of episode. Direction (Left, Up, Right, Down) of first step.\n",
        "    Encoding:\n",
        "        Both observations and labels are OneHot encoded.\n",
        "    Usage:\n",
        "        The two only function a user should need to access are \"construct_numpy_data\" and \"construct_tf_data\"\n",
        "    Options:\n",
        "        Both data construction methods have an option to shuffle the labels of data.\n",
        "        The numpy data construction method allows to also return the maze identifiers.\n",
        "    '''\n",
        "    def __init__(self, goal_presentation_steps, delay_steps, choices_presentation_steps):\n",
        "        self.version = 'v1.2.0'\n",
        "\n",
        "        # Import variables defining episode\n",
        "        self.goal_presentation_steps = goal_presentation_steps\n",
        "        self.delay_steps = delay_steps\n",
        "        self.choices_presentation_steps = choices_presentation_steps\n",
        "\n",
        "        # Construct mazes dataframe\n",
        "        ## Add encoded versions of the goal / choices presentations and the next step response\n",
        "        self.mazesdf = self.import_maze_dic()\n",
        "        self.mazesdf['Goal_Presentation'] = self.mazesdf['goal'].map({\n",
        "            7:np.concatenate((np.array([1,0,0,0]),np.repeat(0,4))),\n",
        "            9:np.concatenate((np.array([0,1,0,0]),np.repeat(0,4))),\n",
        "            17:np.concatenate((np.array([0,0,1,0]),np.repeat(0,4))),\n",
        "            19:np.concatenate((np.array([0,0,0,1]),np.repeat(0,4)))})\n",
        "        self.mazesdf['Choices_Presentation']=self.mazesdf['ChoicesCategory'].map(lambda x: self.encode_choices(x=x))\n",
        "        self.mazesdf['Step_Encoded']=self.mazesdf['NextFPmap'].map(lambda x: self.encode_next_step(x=x))\n",
        "\n",
        "    def construct_numpy_data(self, number_of_problems, return_maze_identifiers = False, np_shuffle_data = False):\n",
        "        # Create a new column which hold the vector for each problem\n",
        "        self.mazesdf['Problem_Vec']=self.mazesdf.apply(lambda x: self.create_problem_observation(row= x,goal_presentation_steps= self.goal_presentation_steps,delay_steps= self.delay_steps,choices_presentation_steps= self.choices_presentation_steps), axis=1)\n",
        "        # Set a random order of maze problems for the current session\n",
        "        self.mazes_order = np.random.randint(0,8,number_of_problems)\n",
        "\n",
        "        # Create vectors, holding observations and labels\n",
        "        session_observation =np.array([])\n",
        "        session_labels = np.array([])\n",
        "        for i in self.mazes_order:\n",
        "            session_observation = np.append(session_observation,self.mazesdf.iloc[i]['Problem_Vec'])\n",
        "            session_labels = np.append(session_labels,self.mazesdf.iloc[i]['Step_Encoded'])\n",
        "\n",
        "        # Reshape vectors to fit network observation and response space\n",
        "        session_length = self.goal_presentation_steps + self.delay_steps + self.choices_presentation_steps\n",
        "        session_observation = np.reshape(session_observation, (-1,session_length,8)).astype('float32')\n",
        "        session_labels = np.reshape(session_labels, (-1,4)).astype('float32')\n",
        "\n",
        "        # If np_shuffle_data == 'Labels, the order of labels is shuffled to randomise correct answers\n",
        "        if np_shuffle_data == 'Labels':\n",
        "          shuffle_generator = np.random.default_rng(38446)\n",
        "          shuffle_generator.shuffle(session_labels,axis=0)\n",
        "\n",
        "        # If return_maze_identifiers == 'IDs', return the array with maze IDs alongside the regular returns (observations, labels)\n",
        "        if return_maze_identifiers == 'IDs':\n",
        "          return session_observation, session_labels, self.mazes_order\n",
        "\n",
        "        return session_observation, session_labels\n",
        "\n",
        "    def construct_tf_data(self, number_of_problems, batch_size, tf_shuffle_data = False):\n",
        "        # Create dataset as described by numpy dataset function and transform it into a TF dataset\n",
        "        npds, np_labels = self.construct_numpy_data(number_of_problems=number_of_problems, np_shuffle_data = tf_shuffle_data)\n",
        "        tfdf = tf.data.Dataset.from_tensor_slices((npds, np_labels))\n",
        "        tfdf = tfdf.batch(batch_size)\n",
        "        return tfdf\n",
        "\n",
        "    def reset_construction_params(self, goal_presentation_steps, delay_steps, choices_presentation_steps):\n",
        "        self.goal_presentation_steps = goal_presentation_steps\n",
        "        self.delay_steps = delay_steps\n",
        "        self.choices_presentation_steps = choices_presentation_steps\n",
        "\n",
        "    def import_maze_dic(self, mazeDic=None):\n",
        "        if mazeDic == None:\n",
        "            # Set up dataframe with first choices of maze task\n",
        "            ## The dictionary was generated using MazeMetadata.py (v1.0.0) and the following call:\n",
        "            ### mazes.loc[(mazes['Nsteps']==2)&(mazes['ChoiceNo']=='ChoiceI')][['goal','ChoicesCategory','NextFPmap']].reset_index(drop=True).to_dict()\n",
        "            self.mazesDic = {'goal': {0: 9, 1: 9, 2: 19, 3: 17, 4: 17, 5: 7, 6: 19, 7: 7},\n",
        "            'ChoicesCategory': {0: 'ul',\n",
        "            1: 'rd',\n",
        "            2: 'ld',\n",
        "            3: 'rd',\n",
        "            4: 'ul',\n",
        "            5: 'ur',\n",
        "            6: 'lr',\n",
        "            7: 'lr'},\n",
        "            'NextFPmap': {0: 'u', 1: 'r', 2: 'd', 3: 'd', 4: 'l', 5: 'u', 6: 'r', 7: 'l'}}\n",
        "        else:\n",
        "            self.mazesDic = mazesDic\n",
        "\n",
        "        # Create and return dataframe\n",
        "        return pd.DataFrame(self.mazesDic)\n",
        "\n",
        "    def encode_choices(self, x):\n",
        "        # Helper function to create the observation vector for choice periods\n",
        "        choices_sec = np.repeat(0,4)\n",
        "        choicesEncoding = pd.Series(list(x))\n",
        "        choicesEncoding = choicesEncoding.map({'l':1,'u':2,'r':3,'d':4})\n",
        "        for encodedChoice in choicesEncoding:\n",
        "            choices_sec[encodedChoice-1]=1\n",
        "        return np.concatenate((np.repeat(0,4),choices_sec))\n",
        "\n",
        "    def encode_next_step(self, x):\n",
        "        # Helper function to change the response / action to a OneHot encoded vector\n",
        "        step_sec = np.repeat(0,4)\n",
        "        stepEncoding = pd.Series(list(x))\n",
        "        stepEncoding = stepEncoding.map({'l':1,'u':2,'r':3,'d':4})\n",
        "        for encodedStep in stepEncoding:\n",
        "            step_sec[encodedStep-1]=1\n",
        "        return step_sec\n",
        "\n",
        "    def create_problem_observation(self, row, goal_presentation_steps, delay_steps, choices_presentation_steps):\n",
        "        # Helper function to create one vector describing the entire outline of one maze problem (Goal presentation, Delay Period, and Choices Presentation)\n",
        "        goal_vec = np.tile(row['Goal_Presentation'], goal_presentation_steps)\n",
        "        delay_vec = np.tile(np.repeat(0,8), delay_steps)\n",
        "        choices_vec = np.tile(row['Choices_Presentation'], choices_presentation_steps)\n",
        "        problem_vec = np.concatenate((goal_vec,delay_vec,choices_vec))\n",
        "        return problem_vec\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '\\n'.join([\n",
        "            f'Maze DataSet Generator',\n",
        "            f'Goal Presentation Steps: {self.goal_presentation_steps}',\n",
        "            f'Delay Steps: {self.delay_steps}',\n",
        "            f'Choices Presentation Steps: {self.choices_presentation_steps}'])\n"
      ],
      "metadata": {
        "id": "f4rZn22NI5U9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate datasets for training"
      ],
      "metadata": {
        "id": "3E_IiQ10JRvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This constructor might run for around a minute to generate the dataset\n",
        "gen = mazeGeneratorI(goal_presentation_steps=20,delay_steps=10,choices_presentation_steps=20)\n",
        "tfdf = gen.construct_tf_data(number_of_problems=5120, batch_size=128)\n",
        "tfdf_test = gen.construct_tf_data(number_of_problems=2560, batch_size=128)\n",
        "tfdf_val = gen.construct_tf_data(number_of_problems=2560, batch_size=128)\n",
        "print(tfdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT-3CUZCJVdI",
        "outputId": "84b01249-a020-4502-dfc5-bbf9402b4511"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(None, 50, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show example of dataset\n",
        "example_data = next(iter(tfdf))\n",
        "#print(example_data)"
      ],
      "metadata": {
        "id": "v6ZYJUbmJn-O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## seRNN"
      ],
      "metadata": {
        "id": "AoJwljfVKBpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regulariser\n",
        "\n",
        "In this section we define two regularisation functions:\n",
        "1. Regulariser for Euclidean embedding\n",
        "2. Subfunction which adds the communicability value (this is the one we use in seRNNs)"
      ],
      "metadata": {
        "id": "W3fxROEHKGVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SE1(Regularizer):\n",
        "  \"\"\"A regulariser for sptially embedded RNNs.\n",
        "  Applies L1 regularisation to recurrent kernel of\n",
        "  RNN which is weighted by the distance of units\n",
        "  in predefined 3D space.\n",
        "  Calculation:\n",
        "      se1 * sum[distance_matrix o recurrent_kernel]\n",
        "  Attributes:\n",
        "      se1: Float; Weighting of SE1 regularisation term.\n",
        "      distance_tensor: TF tensor / matrix with cost per\n",
        "      connection in weight matrix of network.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, se1=0.01, neuron_num = 100, network_structure = (5,5,4), coordinates_list = None, distance_power = 1, distance_metric = 'euclidean'):\n",
        "    self.version = 'v1.1.0'\n",
        "    self.distance_power = distance_power\n",
        "\n",
        "    # Set SE1 regularisation strength to default of 0.01 if no value given\n",
        "    se1 = 0.01 if se1 is None else se1\n",
        "    self._check_penalty_number(se1)\n",
        "\n",
        "    # Transform regularisation strength to TF's standard float format\n",
        "    self.se1 = backend.cast_to_floatx(se1)\n",
        "\n",
        "    # Set up tensor with distance matrix\n",
        "    ## Set up neurons per dimension\n",
        "    nx = np.arange(network_structure[0])\n",
        "    ny = np.arange(network_structure[1])\n",
        "    nz = np.arange(network_structure[2])\n",
        "\n",
        "    ## Set up coordinate grid\n",
        "    [x,y,z] = np.meshgrid(nx,ny,nz)\n",
        "    self.coordinates = [x.ravel(),y.ravel(),z.ravel()]\n",
        "\n",
        "    ## Override coordinate grid if one if provided in init\n",
        "    if coordinates_list!=None:\n",
        "      self.coordinates = coordinates_list\n",
        "\n",
        "    ## Check neuron number / number of coordinates\n",
        "    if (len(self.coordinates[0])==neuron_num)&(len(self.coordinates[1])==neuron_num)&(len(self.coordinates[2])==neuron_num):\n",
        "      pass\n",
        "    else:\n",
        "      raise ValueError('Network / coordinate structure does not match the number of neurons.')\n",
        "\n",
        "    ## Calculate the euclidean distance matrix\n",
        "    euclidean_vector = scipy.spatial.distance.pdist(np.transpose(self.coordinates), metric=distance_metric)\n",
        "    euclidean = scipy.spatial.distance.squareform(euclidean_vector**self.distance_power)\n",
        "    self.distance_matrix = euclidean.astype('float32')\n",
        "    self.spatial_cost_matrix = self.distance_matrix\n",
        "\n",
        "    ## Add minimal cost for recurrent self connection (on diagonal)\n",
        "    #diag_dist = np.diag(np.repeat(0.1,100)).astype('float32')\n",
        "    #self.distance_matrix = self.distance_matrix + diag_dist\n",
        "\n",
        "    ## Create tensor from distance matrix\n",
        "    self.distance_tensor =  tf.convert_to_tensor(self.distance_matrix)\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # Add calculation of loss here.\n",
        "    # L1 for reference: self.l1 * math_ops.reduce_sum(math_ops.abs(x))\n",
        "    abs_weight_matrix = tf.math.abs(x)\n",
        "\n",
        "    #se1_loss = self.se1 * tf.math.multiply(abs_weight_matrix, self.distance_tensor)\n",
        "    #se1_loss = tf.math.reduce_sum(abs_weight_matrix)\n",
        "    se1_loss = self.se1 * tf.math.reduce_sum(tf.math.multiply(abs_weight_matrix, self.distance_tensor))\n",
        "\n",
        "    return se1_loss\n",
        "\n",
        "  def _check_penalty_number(self, x):\n",
        "    \"\"\"check penalty number availability, raise ValueError if failed.\"\"\"\n",
        "    if not isinstance(x, (float, int)):\n",
        "      raise ValueError(('Value: {} is not a valid regularization penalty number, '\n",
        "                        'expected an int or float value').format(x))\n",
        "\n",
        "  def visualise_distance_matrix(self):\n",
        "    plt.imshow(self.distance_matrix)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "  def visualise_neuron_structure(self):\n",
        "    fig = plt.figure()\n",
        "    ax = Axes3D(fig)\n",
        "    ax.scatter(self.coordinates[0],self.coordinates[1],self.coordinates[2],c='b',marker='.')\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_zlabel('z')\n",
        "    plt.show()\n",
        "\n",
        "  def get_config(self):\n",
        "    return {'se1': float(self.se1)}"
      ],
      "metadata": {
        "id": "6jsCMjDWK6uF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SE1_sWc(SE1):\n",
        "    '''\n",
        "    Version of SE1 regulariser which combines the spatial and communicability parts in loss function.\n",
        "    Additional comms_factor scales the communicability matrix.\n",
        "    The communicability term used here is unbiased weighted communicability:\n",
        "    Crofts, J. J., & Higham, D. J. (2009). A weighted communicability measure applied to complex brain networks. Journal of the Royal Society Interface, 6(33), 411-414.\n",
        "    '''\n",
        "    def __init__(self, se1=0.01, comms_factor = 1, neuron_num = 100, network_structure = (5,5,4), coordinates_list = None, distance_power = 1, distance_metric = 'euclidean'):\n",
        "      SE1.__init__(self, se1, neuron_num , network_structure , coordinates_list, distance_power , distance_metric)\n",
        "      self.comms_factor = comms_factor\n",
        "\n",
        "    def __call__(self, x):\n",
        "      # Take absolute of weights\n",
        "      abs_weight_matrix = tf.math.abs(x)\n",
        "\n",
        "      # Calulcate weighted communicability (see reference in docstring)\n",
        "      stepI = tf.math.reduce_sum(abs_weight_matrix, axis=1)\n",
        "      stepII = tf.math.pow(stepI, -0.5)\n",
        "      stepIII = tf.linalg.diag(stepII)\n",
        "      stepIV = tf.linalg.expm(stepIII@abs_weight_matrix@stepIII)\n",
        "      comms_matrix = tf.linalg.set_diag(stepIV, tf.zeros(stepIV.shape[0:-1]))\n",
        "\n",
        "      # Multiply absolute weights with communicability weights\n",
        "      comms_matrix = comms_matrix**self.comms_factor\n",
        "      comms_weight_matrix = tf.math.multiply(abs_weight_matrix, comms_matrix)\n",
        "\n",
        "      # Multiply comms weights matrix with distance tensor, scale the mean, and return as loss\n",
        "      se1_loss = self.se1 * tf.math.reduce_sum(tf.math.multiply(comms_weight_matrix , self.distance_tensor))\n",
        "\n",
        "      return se1_loss"
      ],
      "metadata": {
        "id": "hLVlbpnkK_i2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training helper functions\n",
        "\n",
        "Here we define a callback to give us easy access to weight matrices after training."
      ],
      "metadata": {
        "id": "XtsBAdHCQG12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNWeightMatrixHistoryI(keras.callbacks.Callback):\n",
        "    '''\n",
        "    Saves the RNN_Weight_Matrix to the training history before\n",
        "    the start of training and after finishing each epoch.\n",
        "\n",
        "    The network model needs to be build manually before calling fit() method\n",
        "    for this callback to work.\n",
        "    '''\n",
        "    def __init__(self, RNN_layer_number = 0):\n",
        "        super(RNNWeightMatrixHistoryI, self).__init__()\n",
        "        self.RNN_layer_number = RNN_layer_number\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # Create key for RNN_Weight_Matrix in history\n",
        "        self.model.history.history['RNN_Weight_Matrix'] = []\n",
        "        #print(\"Created key for RNN_Weight_Matrix in history.\")\n",
        "\n",
        "        # Save matrix before start of training\n",
        "        self.model.history.history['RNN_Weight_Matrix'].append(self.model.layers[self.RNN_layer_number].get_weights()[1])\n",
        "        #print(\"Saved RNN_Weight_Matrix to history.\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save RNN_Weight_Matrix to history\n",
        "        self.model.history.history['RNN_Weight_Matrix'].append(self.model.layers[self.RNN_layer_number].get_weights()[1])\n",
        "        #print(\"Saved RNN_Weight_Matrix to history.\")"
      ],
      "metadata": {
        "id": "tNx_GNBcQSzb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model defintion"
      ],
      "metadata": {
        "id": "8aQQSb6SKMLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example regularisation strength from set of networks used in preprint\n",
        "regu_strength = 0.5\n",
        "print(regu_strength)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC04f--UKOat",
        "outputId": "975bc02d-82f0-4452-c27d-43b44fd26160"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "regu = SE1_sWc(se1=regu_strength)\n",
        "coord = regu.coordinates\n",
        "cost = regu.spatial_cost_matrix\n",
        "\n",
        "## Assemble network\n",
        "tf_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.GaussianNoise(stddev=0.05),\n",
        "    tf.keras.layers.SimpleRNN(100, activation='relu',recurrent_initializer='orthogonal', return_sequences=False, recurrent_regularizer= regu),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "tf_model.build(input_shape=example_data[0].shape)"
      ],
      "metadata": {
        "id": "FMS3FkPQKWOR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training"
      ],
      "metadata": {
        "id": "3q7MSHLzolA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ltW_nLrgO8ff"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = tf_model.fit(tfdf, epochs=10,validation_data=tfdf_test,\n",
        "                       callbacks=RNNWeightMatrixHistoryI(RNN_layer_number=1)\n",
        "                       )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k3pOnJ-PFUQ",
        "outputId": "60ef460b-135c-4925-d6d7-79e80b884cb2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.5611 - loss: 26.7109 - val_accuracy: 0.6090 - val_loss: 20.7074\n",
            "Epoch 2/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.6364 - loss: 19.0234 - val_accuracy: 0.6230 - val_loss: 14.5047\n",
            "Epoch 3/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.6379 - loss: 13.2064 - val_accuracy: 0.6180 - val_loss: 9.5570\n",
            "Epoch 4/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6323 - loss: 8.4693 - val_accuracy: 0.6180 - val_loss: 5.4120\n",
            "Epoch 5/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.6412 - loss: 4.5253 - val_accuracy: 0.6180 - val_loss: 2.1470\n",
            "Epoch 6/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.6419 - loss: 1.6174 - val_accuracy: 0.6230 - val_loss: 0.6994\n",
            "Epoch 7/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6253 - loss: 0.6394 - val_accuracy: 0.6141 - val_loss: 0.5580\n",
            "Epoch 8/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.6111 - loss: 0.5554 - val_accuracy: 0.6141 - val_loss: 0.5451\n",
            "Epoch 9/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.6260 - loss: 0.5443 - val_accuracy: 0.6141 - val_loss: 0.5385\n",
            "Epoch 10/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - accuracy: 0.6174 - loss: 0.5396 - val_accuracy: 0.6141 - val_loss: 0.5349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Structural analysis"
      ],
      "metadata": {
        "id": "O-OEkjfIookA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install bctpy\n",
        "import bct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftpn1KPsPs_b",
        "outputId": "606efce1-580e-4983-a9f0-2568d61034d3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bctpy\n",
            "  Downloading bctpy-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bctpy) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from bctpy) (1.15.3)\n",
            "Downloading bctpy-0.6.1-py3-none-any.whl (110 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/110.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m102.4/110.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.4/110.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bctpy\n",
            "Successfully installed bctpy-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract example absolute weight matrix from trained network\n",
        "history_dic = history.history\n",
        "example_weight_matrix = np.abs(history_dic['RNN_Weight_Matrix'][10])"
      ],
      "metadata": {
        "id": "zpL1R-8XRHDF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarise network before structural analysis\n",
        "binary_weight_matrix = example_weight_matrix.copy()\n",
        "thresh = np.quantile(example_weight_matrix, q=0.9)\n",
        "matrix_mask = example_weight_matrix > thresh\n",
        "binary_weight_matrix[matrix_mask] = 1\n",
        "binary_weight_matrix[~matrix_mask] = 0\n",
        "#binary_weight_matrix"
      ],
      "metadata": {
        "id": "VvcX8Uy1S1IK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Modularity\n",
        "\n",
        "Note that we used this demo before but that we added a manual modularity function now as the modularity function was not compatible with the current version of python hosted on Colab."
      ],
      "metadata": {
        "id": "PTC37RmnSEVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#_, q_stat = bct.modularity_und(binary_weight_matrix, gamma=1)\n",
        "#print(q_stat)"
      ],
      "metadata": {
        "id": "ck6_y-dtWmAy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modularity_und(binary_weight_matrix, gamma=1):\n",
        "    \"\"\"\n",
        "    Manual modularity calculation using Louvain algorithm\n",
        "    Returns: (communities, modularity_value)\n",
        "    \"\"\"\n",
        "    A = np.array(binary_weight_matrix, dtype=np.float64)\n",
        "    A = (A + A.T) / 2  # Make symmetric\n",
        "    A = (A > 0).astype(np.float64)  # Ensure binary\n",
        "\n",
        "    n = A.shape[0]\n",
        "    m = np.sum(A) / 2  # Total edges\n",
        "\n",
        "    if m == 0:\n",
        "        return np.arange(n), 0.0\n",
        "\n",
        "    degrees = np.sum(A, axis=1)\n",
        "    communities = np.arange(n)  # Start with each node in own community\n",
        "\n",
        "    # Louvain algorithm\n",
        "    improved = True\n",
        "    while improved:\n",
        "        improved = False\n",
        "        for node in np.random.permutation(n):\n",
        "            current_comm = communities[node]\n",
        "            best_comm = current_comm\n",
        "            best_gain = 0.0\n",
        "\n",
        "            # Check neighboring communities\n",
        "            neighbors = np.where(A[node] > 0)[0]\n",
        "            neighbor_comms = set(communities[neighbors])\n",
        "\n",
        "            for new_comm in neighbor_comms:\n",
        "                if new_comm == current_comm:\n",
        "                    continue\n",
        "\n",
        "                # Calculate modularity gain\n",
        "                gain = 0.0\n",
        "                for j in range(n):\n",
        "                    if j != node:\n",
        "                        if communities[j] == new_comm:\n",
        "                            gain += 2 * (A[node, j] - gamma * degrees[node] * degrees[j] / (2 * m))\n",
        "                        elif communities[j] == current_comm:\n",
        "                            gain -= 2 * (A[node, j] - gamma * degrees[node] * degrees[j] / (2 * m))\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_comm = new_comm\n",
        "\n",
        "            if best_gain > 0:\n",
        "                communities[node] = best_comm\n",
        "                improved = True\n",
        "\n",
        "    # Calculate final modularity\n",
        "    Q = 0.0\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if communities[i] == communities[j]:\n",
        "                Q += A[i, j] - gamma * (degrees[i] * degrees[j]) / (2 * m)\n",
        "    Q = Q / (2 * m)\n",
        "\n",
        "    return communities, Q\n",
        "\n",
        "# Replace your broken bctpy line with this:\n",
        "ci, q_stat = modularity_und(binary_weight_matrix, gamma=1)\n",
        "print(q_stat)"
      ],
      "metadata": {
        "id": "u_iB-4v3dSoq",
        "outputId": "73a27e18-83c9-42f6-8f46-220475a4b1f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19908102416546747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Small-worldness"
      ],
      "metadata": {
        "id": "0KDJ0-47SKBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Empirical clustering and path length\n",
        "A = binary_weight_matrix\n",
        "clu = np.mean(bct.clustering_coef_bu(A))\n",
        "pth = bct.efficiency_bin(A)\n",
        "# Run nperm null models\n",
        "nperm = 1000\n",
        "cluperm = np.zeros((nperm,1))\n",
        "pthperm = np.zeros((nperm,1))\n",
        "for perm in range(nperm):\n",
        "    Wperm = np.random.rand(100,100)\n",
        "    # Make it into a matrix\n",
        "    Wperm = np.matrix(Wperm)\n",
        "    # Make symmetrical\n",
        "    Wperm = Wperm+Wperm.T\n",
        "    Wperm = np.divide(Wperm,2)\n",
        "    # Binarise\n",
        "    threshold, upper, lower = .7,1,0\n",
        "    Aperm = np.where(Wperm>threshold,upper,lower)\n",
        "    # Take null model\n",
        "    cluperm[perm] = np.mean(bct.clustering_coef_bu(Aperm))\n",
        "    pthperm[perm] = bct.efficiency_bin(Aperm)\n",
        "# Take the average of the nulls\n",
        "clunull = np.mean(cluperm)\n",
        "pthnull = np.mean(pthperm)\n",
        "# Compute the small worldness\n",
        "smw = np.divide(np.divide(clu,clunull),np.divide(pthnull,pth))\n",
        "\n",
        "print(smw)"
      ],
      "metadata": {
        "id": "zrGtmq1-Q-Uu",
        "outputId": "8b0be4db-8401-4e14-c433-a5890091324a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2272514760623212\n"
          ]
        }
      ]
    }
  ]
}